{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pump It Up Final",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1H_VGW9Qu2KowT7WQm5FjJcxCJzKVIluG",
      "authorship_tag": "ABX9TyP/3LhZLfQ0fZyB9VyUsAYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Per-Y/Driven_Data-Pump_It_Up/blob/master/Pump_It_Up_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYsYSrJWR3xt",
        "colab_type": "text"
      },
      "source": [
        "# Driven Data - Pump it Up\n",
        "## Initial Submission Notebook\n",
        "\n",
        "Latest Edit : *5-Sep-2020*\n",
        "\n",
        "In this notebook, I have explained the process of making a first submission to the Driven Data Challenge '[Pump it Up](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/)'. This notebook will be followed up with a second notebook which will do hyperparameter optimization for the models used here and the results will be compared.\n",
        "\n",
        "TBU:\n",
        "\n",
        "1.   Background\n",
        "2.   Exploratory Data Analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6sqwGlZhmTH",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing of Data - Basics** \n",
        "\n",
        "In this section, some basic actions are taken :\n",
        "\n",
        "1.   Import the dataset csv files as dataframes\n",
        "2.   Combine the training and test data for preprocessing\n",
        "3.   Assign Numerical Values to the 'Status Group' target category to suit training of models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Xk__viW8Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Basic Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Pump It Up/Data Values_Train.csv')\n",
        "train_labels = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Pump It Up/Data Labels_Train.csv')\n",
        "test_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Pump It Up/Data Values_Test.csv')\n",
        "\n",
        "# Combine train and test data\n",
        "train_data['train'] = 1\n",
        "test_data['train'] = 0\n",
        "combined = pd.concat([train_data,test_data])\n",
        "\n",
        "# assign numerical labels\n",
        "label_dict_status_group = {'functional':0, 'non functional': 1, 'functional needs repair': 2}\n",
        "train_labels.status_group = train_labels.status_group.replace(label_dict_status_group)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoZlXT69inCn",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing of Data - Advanced**\n",
        "\n",
        "In this section we do a few crucial actions :\n",
        "\n",
        "1.   Remove the redundant features as identified in the previous section which includes\n",
        "    *   Columns with several NaN values\n",
        "    *   Columns with duplicate information\n",
        "    *   Columns with a single value for all rows\n",
        "    *   Columns with coordinates (other features act as a proxy for location information)\n",
        "2.   Convert True/False boolean statements to integer type\n",
        "3.   Perform one-hot encoding of categorical features (only top 25 most common values of each feature to be used)\n",
        "4.   Merging Test Data with Labels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDoxv7A3ir6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dropping Redundant Features (ie. columns with too many blanks, duplicate information, single value for all rows, coordinates)\n",
        "combined = combined.drop(['source_type','date_recorded', 'source_class', 'waterpoint_type_group', 'longitude','latitude', 'quantity_group', 'num_private', 'subvillage', 'region','scheme_name','recorded_by','extraction_type'],axis=1)\n",
        "\n",
        "# Converting Booleans to Integer Values\n",
        "combined.permit = combined.permit.astype(bool).astype(int)\n",
        "combined.public_meeting = combined.public_meeting.astype(bool).astype(int)\n",
        "\n",
        "# List of columns to be one-hot encoded\n",
        "cat_columns = ['funder', 'installer', 'wpt_name', 'basin', 'region_code', 'district_code', 'lga', 'ward', 'public_meeting', 'scheme_management', 'permit', 'extraction_type_group', 'extraction_type_class', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'source', 'waterpoint_type']\n",
        "\n",
        "# Selecting only the top 25 values per column for one-hot encoding\n",
        "for col in cat_columns:\n",
        "  # List of top 25 values for the feature 'col'\n",
        "  top = [x for x in combined[col].value_counts().iloc[:25].index]\n",
        "  for val in top:\n",
        "    # Manual one-hot encoding for each value 'val' in the top 25 values\n",
        "    combined[col + \"_\" + str(val)]=np.where(combined[col]==val,1,0)\n",
        "\n",
        "# Dropping original columns after encoding\n",
        "combined.drop(cat_columns, axis=1, inplace=True)\n",
        "\n",
        "# Separating test and train data\n",
        "train_data=combined[combined['train']==1]\n",
        "test_data=combined[combined['train']==0]\n",
        "train_data = train_data.drop(['train'],axis=1)\n",
        "test_data = test_data.drop(['train'],axis=1)\n",
        "\n",
        "# Merging labels with train data\n",
        "train_data = pd.merge(train_data, train_labels, on = 'id')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C__wC0LzW7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec346a40-ae7f-44c8-cf12-daa0baf40416"
      },
      "source": [
        "# Cross checking if any Nulls remain\n",
        "train_data.isnull().sum().sum()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U88bbQcjxvzy",
        "colab_type": "text"
      },
      "source": [
        "**Modeling - Unoptimized**\n",
        "\n",
        "Here we attempt to run a variety of standard classifiers on our dataset. First we shall import all the necessary libraries then proceed to use the following models :\n",
        "\n",
        "1.   XGBoost Classifier\n",
        "2.   Logistic Regression Classifier\n",
        "3.   Support Vector Classifier\n",
        "4.   Tensorflow Classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_50XMcKXJjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Global libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, RandomizedSearchCV, KFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "#Creating test-train split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(columns=['status_group','id']), train_data['status_group'],test_size=0.33, random_state=420)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlya7oeszo9Q",
        "colab_type": "text"
      },
      "source": [
        "**XGBoost Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzLxRseYznvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "2240c09e-f46b-4828-c8bb-d910ecdaf564"
      },
      "source": [
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "clf_xgb = XGBClassifier(objective='multi:softmax')\n",
        "\n",
        "start_time = time.time()\n",
        "clf_xgb.fit(X_train,y_train)\n",
        "\n",
        "print(\"System took %s seconds to model\" % (time.time() - start_time))\n",
        "print(classification_report(y_test,clf_xgb.predict(X_test)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "System took 69.59227991104126 seconds to model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.94      0.81     10659\n",
            "           1       0.85      0.60      0.71      7498\n",
            "           2       0.81      0.09      0.17      1445\n",
            "\n",
            "    accuracy                           0.75     19602\n",
            "   macro avg       0.79      0.55      0.56     19602\n",
            "weighted avg       0.77      0.75      0.72     19602\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eze2wK-fb3Iu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "424a983c-0216-41d0-ca03-0c851e93bb48"
      },
      "source": [
        "accuracy = accuracy_score(y_test, clf_xgb.predict(X_test))\n",
        "print (\"SCORE:\", accuracy)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORE: 0.7490052035506581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ08QmJvbhPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16aac5f1-31f9-4551-a031-2a9e613ccf7f"
      },
      "source": [
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "\n",
        "space={\n",
        "    'max_depth' : hp.choice('max_depth', range(3, 12, 1)),\n",
        "    'learning_rate' : hp.quniform('learning_rate',0.05, 1, 0.05),\n",
        "    'n_estimators' : hp.choice('n_estimators', range(50,350,10)),\n",
        "    'gamma' : hp.quniform('gamma', 0, 5, 1),\n",
        "    'reg_alpha' : hp.quniform('reg_alpha', 0, 50, 0.5),\n",
        "    'min_child_weight' : hp.quniform('min_child_weight', 1, 20, 1),\n",
        "    'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
        "    'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1.0, 0.1)\n",
        "    }\n",
        "def objective(space):\n",
        "    clf=XGBClassifier(\n",
        "                    n_estimators =space['n_estimators'], \n",
        "                    max_depth = int(space['max_depth']), \n",
        "                    gamma = space['gamma'],\n",
        "                    reg_alpha = int(space['reg_alpha']),\n",
        "                    min_child_weight=int(space['min_child_weight']),\n",
        "                    colsample_bytree=int(space['colsample_bytree']),\n",
        "                    learning_rate=space['learning_rate'],\n",
        "                    objective='multi:softmax',\n",
        "                    subsample=space['subsample'])\n",
        "    \n",
        "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
        "    \n",
        "    clf.fit(X_train, y_train,\n",
        "            eval_set=evaluation, eval_metric=\"mlogloss\",\n",
        "            early_stopping_rounds=10,verbose=False)\n",
        "    \n",
        "\n",
        "    pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, pred)\n",
        "    print (\"SCORE:\", accuracy)\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
        "\n",
        "\n",
        "trials = Trials()\n",
        "best_hyperparams = fmin(fn = objective,\n",
        "                        space = space,\n",
        "                        algo = tpe.suggest,\n",
        "                        max_evals = 100,\n",
        "                        trials = trials)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORE:\n",
            "0.6909499030711151\n",
            "SCORE:\n",
            "0.6869707172737476\n",
            "SCORE:\n",
            "0.6901846750331598\n",
            "SCORE:\n",
            "0.7140087746148353\n",
            "SCORE:\n",
            "0.7210488725640241\n",
            "SCORE:\n",
            "0.7072237526782982\n",
            "SCORE:\n",
            "0.713549637792062\n",
            "SCORE:\n",
            "0.6926844199571472\n",
            "SCORE:\n",
            "0.7194674012855831\n",
            "SCORE:\n",
            "0.7747678808284869\n",
            "SCORE:\n",
            "0.6969186817671666\n",
            "SCORE:\n",
            "0.7136006529945924\n",
            "SCORE:\n",
            "0.6301907968574635\n",
            "SCORE:\n",
            "0.7029384756657484\n",
            "SCORE:\n",
            "0.6412610958065503\n",
            "SCORE:\n",
            "0.6676359555147434\n",
            "SCORE:\n",
            "0.6791653912866034\n",
            "SCORE:\n",
            "0.6613100704009794\n",
            "SCORE:\n",
            "0.7019181716151414\n",
            "SCORE:\n",
            "0.6996735027038057\n",
            "SCORE:\n",
            "0.7784919906132027\n",
            "SCORE:\n",
            "0.7755331088664422\n",
            "SCORE:\n",
            "0.7803795531068258\n",
            "SCORE:\n",
            "0.7800224466891134\n",
            "SCORE:\n",
            "0.7805325987144169\n",
            "SCORE:\n",
            "0.7768595041322314\n",
            "SCORE:\n",
            "0.7714518926640139\n",
            "SCORE:\n",
            "0.7866544230180594\n",
            "SCORE:\n",
            "0.7165085195388226\n",
            "SCORE:\n",
            "0.7169676563615958\n",
            "SCORE:\n",
            "0.7784409754106724\n",
            "SCORE:\n",
            "0.7114580144883175\n",
            "SCORE:\n",
            "0.7229364350576472\n",
            "SCORE:\n",
            "0.75216814610754\n",
            "SCORE:\n",
            "0.7026323844505663\n",
            "SCORE:\n",
            "0.7232935414753596\n",
            "SCORE:\n",
            "0.7117641057034997\n",
            "SCORE:\n",
            "0.6549331700846852\n",
            "SCORE:\n",
            "0.7733904703601673\n",
            "SCORE:\n",
            "0.721610039791858\n",
            "SCORE:\n",
            "0.7194674012855831\n",
            "SCORE:\n",
            "0.7041628405264769\n",
            "SCORE:\n",
            "0.7503315988164473\n",
            "SCORE:\n",
            "0.6993674114886236\n",
            "SCORE:\n",
            "0.7033465972859912\n",
            "SCORE:\n",
            "0.7539026629935721\n",
            "SCORE:\n",
            "0.6872768084889297\n",
            "SCORE:\n",
            "0.7186001428425671\n",
            "SCORE:\n",
            "0.7221201918171615\n",
            "SCORE:\n",
            "0.7086011631466177\n",
            "SCORE:\n",
            "0.697836955412713\n",
            "SCORE:\n",
            "0.7087542087542088\n",
            "SCORE:\n",
            "0.7791551882460973\n",
            "SCORE:\n",
            "0.7910927456382002\n",
            "SCORE:\n",
            "0.7043669013365983\n",
            "SCORE:\n",
            "0.7191102948678706\n",
            "SCORE:\n",
            "0.7898683807774717\n",
            "SCORE:\n",
            "0.6601367207427814\n",
            "SCORE:\n",
            "0.6913070094888276\n",
            "SCORE:\n",
            "0.7745128048158351\n",
            "SCORE:\n",
            "0.7827772676257525\n",
            "SCORE:\n",
            "0.6830425466789103\n",
            "SCORE:\n",
            "0.6969186817671666\n",
            "SCORE:\n",
            "0.6954392408937864\n",
            "SCORE:\n",
            "0.7150800938679727\n",
            "SCORE:\n",
            "0.7776247321701867\n",
            "SCORE:\n",
            "0.7679318436894195\n",
            "SCORE:\n",
            "0.7614529129680645\n",
            "SCORE:\n",
            "0.7641567187021733\n",
            "SCORE:\n",
            "0.7517600244872972\n",
            "SCORE:\n",
            "0.7126313641465156\n",
            "SCORE:\n",
            "0.7703805734108764\n",
            "SCORE:\n",
            "0.7143658810325477\n",
            "SCORE:\n",
            "0.7908886848280787\n",
            "SCORE:\n",
            "0.760228548107336\n",
            "SCORE:\n",
            "0.7066625854504642\n",
            "SCORE:\n",
            "0.7791551882460973\n",
            "SCORE:\n",
            "0.7058463422099786\n",
            "SCORE:\n",
            "0.7729313335373942\n",
            "SCORE:\n",
            "0.7047240077543108\n",
            "SCORE:\n",
            "0.7580348943985308\n",
            "SCORE:\n",
            "0.6900316294255688\n",
            "SCORE:\n",
            "0.7761452912968064\n",
            "SCORE:\n",
            "0.6938577696153454\n",
            "SCORE:\n",
            "0.7621671258034894\n",
            "SCORE:\n",
            "0.709366391184573\n",
            "SCORE:\n",
            "0.7088052239567391\n",
            "SCORE:\n",
            "0.7669625548413427\n",
            "SCORE:\n",
            "0.7224772982348739\n",
            "SCORE:\n",
            "0.7010509131721253\n",
            "SCORE:\n",
            "0.6770737679828589\n",
            "SCORE:\n",
            "0.7573716967656362\n",
            "SCORE:\n",
            "0.71900826446281\n",
            "SCORE:\n",
            "0.7824711764105704\n",
            "SCORE:\n",
            "0.6956433017039078\n",
            "SCORE:\n",
            "0.7303336394245485\n",
            "SCORE:\n",
            "0.7205897357412508\n",
            "SCORE:\n",
            "0.7658912355882053\n",
            "SCORE:\n",
            "0.7149780634629119\n",
            "SCORE:\n",
            "0.7800224466891134\n",
            "100%|██████████| 100/100 [4:50:07<00:00, 174.07s/it, best loss: -0.7910927456382002]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--GX8mfxmaEk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "a21ed0c1-894a-4aa8-e1c4-2fa4400b602e"
      },
      "source": [
        "best_hyperparams"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'colsample_bytree': 1.0,\n",
              " 'gamma': 0.0,\n",
              " 'learning_rate': 0.75,\n",
              " 'max_depth': 8,\n",
              " 'min_child_weight': 16.0,\n",
              " 'n_estimators': 22,\n",
              " 'reg_alpha': 33.0,\n",
              " 'subsample': 0.8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe3jC8xMJk3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "0de04910-cb99-4328-b20a-0dd399e421e3"
      },
      "source": [
        "clf_xgb = XGBClassifier(objective='multi:softmax',\n",
        "                        colsample_bytree=1.0,\n",
        "                        gamma=0.0,\n",
        "                        learning_rate=0.4,\n",
        "                        max_depth=6,\n",
        "                        min_child_weight=1.0,\n",
        "                        n_estimators=11,\n",
        "                        reg_alpha=9.0,\n",
        "                        subsample=0.7000000000000001,\n",
        "                        )\n",
        "\n",
        "start_time = time.time()\n",
        "clf_xgb.fit(X_train,y_train)\n",
        "\n",
        "print(\"System took %s seconds to model\" % (time.time() - start_time))\n",
        "print(classification_report(y_test,clf_xgb.predict(X_test)))\n",
        "\n",
        "# Finding the predictions from the models on the final test data\n",
        "xgb_pred = pd.DataFrame({'status_group' : clf_xgb.predict(test_data.drop(['id'],axis=1))})\n",
        "\n",
        "# Adding the id column back to the predicitions\n",
        "id_df = pd.DataFrame(test_data['id'])\n",
        "xgb_pred = id_df.merge(xgb_pred,left_index=True,right_index=True)\n",
        "\n",
        "# Creating Submission Files\n",
        "label_dict_status_group = {0:'functional', 1:'non functional', 2:'functional needs repair'}\n",
        "xgb_pred.status_group = xgb_pred.status_group.replace(label_dict_status_group)\n",
        "\n",
        "# Exporting the data to csv files for submission\n",
        "pd.DataFrame(xgb_pred).to_csv('xgb_pred_opt.csv',index=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "System took 19.506816625595093 seconds to model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.93      0.81     10659\n",
            "           1       0.84      0.63      0.72      7498\n",
            "           2       0.74      0.11      0.20      1445\n",
            "\n",
            "    accuracy                           0.76     19602\n",
            "   macro avg       0.77      0.56      0.58     19602\n",
            "weighted avg       0.77      0.76      0.73     19602\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPqUY5dV4Iyg",
        "colab_type": "text"
      },
      "source": [
        "**Support Vector Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr77Dqsz6Bv7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad1e06ec-2dda-404f-d312-58d58bdf97bc"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import reciprocal, uniform\n",
        "\n",
        "clf_svm = LinearSVC()\n",
        "\n",
        "param_distributions = {\"max_iter\": uniform(500, 1500), \"C\": uniform(1, 20)}\n",
        "rnd_search_cv = RandomizedSearchCV(clf_svm, param_distributions, n_iter=10, verbose=2, cv=3)\n",
        "rnd_search_cv.fit(X_train, y_train)\n",
        "\n",
        "start_time = time.time()\n",
        "clf_svm.fit(X_train,y_train)\n",
        "\n",
        "print(\"System took %s seconds to model\" % (time.time() - start_time))\n",
        "print(classification_report(y_test,clf_svm.predict(X_test)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] C=16.6246997315387, max_iter=1251.0460575266338 .................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .. C=16.6246997315387, max_iter=1251.0460575266338, total=  14.4s\n",
            "[CV] C=16.6246997315387, max_iter=1251.0460575266338 .................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .. C=16.6246997315387, max_iter=1251.0460575266338, total=  13.9s\n",
            "[CV] C=16.6246997315387, max_iter=1251.0460575266338 .................\n",
            "[CV] .. C=16.6246997315387, max_iter=1251.0460575266338, total=  15.2s\n",
            "[CV] C=13.847264010980407, max_iter=1698.0557296001612 ...............\n",
            "[CV]  C=13.847264010980407, max_iter=1698.0557296001612, total=  20.4s\n",
            "[CV] C=13.847264010980407, max_iter=1698.0557296001612 ...............\n",
            "[CV]  C=13.847264010980407, max_iter=1698.0557296001612, total=  20.8s\n",
            "[CV] C=13.847264010980407, max_iter=1698.0557296001612 ...............\n",
            "[CV]  C=13.847264010980407, max_iter=1698.0557296001612, total=  20.0s\n",
            "[CV] C=5.438227931143861, max_iter=593.0844584177224 .................\n",
            "[CV] .. C=5.438227931143861, max_iter=593.0844584177224, total=   7.5s\n",
            "[CV] C=5.438227931143861, max_iter=593.0844584177224 .................\n",
            "[CV] .. C=5.438227931143861, max_iter=593.0844584177224, total=   6.7s\n",
            "[CV] C=5.438227931143861, max_iter=593.0844584177224 .................\n",
            "[CV] .. C=5.438227931143861, max_iter=593.0844584177224, total=   6.8s\n",
            "[CV] C=17.45544316347841, max_iter=1946.9116718461016 ................\n",
            "[CV] . C=17.45544316347841, max_iter=1946.9116718461016, total=  22.4s\n",
            "[CV] C=17.45544316347841, max_iter=1946.9116718461016 ................\n",
            "[CV] . C=17.45544316347841, max_iter=1946.9116718461016, total=  22.2s\n",
            "[CV] C=17.45544316347841, max_iter=1946.9116718461016 ................\n",
            "[CV] . C=17.45544316347841, max_iter=1946.9116718461016, total=  20.0s\n",
            "[CV] C=12.382704890888306, max_iter=639.2763727350191 ................\n",
            "[CV] . C=12.382704890888306, max_iter=639.2763727350191, total=   6.4s\n",
            "[CV] C=12.382704890888306, max_iter=639.2763727350191 ................\n",
            "[CV] . C=12.382704890888306, max_iter=639.2763727350191, total=   6.8s\n",
            "[CV] C=12.382704890888306, max_iter=639.2763727350191 ................\n",
            "[CV] . C=12.382704890888306, max_iter=639.2763727350191, total=   6.8s\n",
            "[CV] C=16.276500115025605, max_iter=529.9887986164601 ................\n",
            "[CV] . C=16.276500115025605, max_iter=529.9887986164601, total=   5.4s\n",
            "[CV] C=16.276500115025605, max_iter=529.9887986164601 ................\n",
            "[CV] . C=16.276500115025605, max_iter=529.9887986164601, total=   5.5s\n",
            "[CV] C=16.276500115025605, max_iter=529.9887986164601 ................\n",
            "[CV] . C=16.276500115025605, max_iter=529.9887986164601, total=   5.2s\n",
            "[CV] C=12.141517356786082, max_iter=1647.598600702512 ................\n",
            "[CV] . C=12.141517356786082, max_iter=1647.598600702512, total=  16.8s\n",
            "[CV] C=12.141517356786082, max_iter=1647.598600702512 ................\n",
            "[CV] . C=12.141517356786082, max_iter=1647.598600702512, total=  16.6s\n",
            "[CV] C=12.141517356786082, max_iter=1647.598600702512 ................\n",
            "[CV] . C=12.141517356786082, max_iter=1647.598600702512, total=  16.8s\n",
            "[CV] C=13.823317942640468, max_iter=858.9672031120147 ................\n",
            "[CV] . C=13.823317942640468, max_iter=858.9672031120147, total=   8.6s\n",
            "[CV] C=13.823317942640468, max_iter=858.9672031120147 ................\n",
            "[CV] . C=13.823317942640468, max_iter=858.9672031120147, total=   8.2s\n",
            "[CV] C=13.823317942640468, max_iter=858.9672031120147 ................\n",
            "[CV] . C=13.823317942640468, max_iter=858.9672031120147, total=   9.3s\n",
            "[CV] C=10.6898733136027, max_iter=948.3478763001834 ..................\n",
            "[CV] ... C=10.6898733136027, max_iter=948.3478763001834, total=   9.9s\n",
            "[CV] C=10.6898733136027, max_iter=948.3478763001834 ..................\n",
            "[CV] ... C=10.6898733136027, max_iter=948.3478763001834, total=   9.9s\n",
            "[CV] C=10.6898733136027, max_iter=948.3478763001834 ..................\n",
            "[CV] ... C=10.6898733136027, max_iter=948.3478763001834, total=  10.4s\n",
            "[CV] C=1.102112922044213, max_iter=1725.1867162187282 ................\n",
            "[CV] . C=1.102112922044213, max_iter=1725.1867162187282, total=  18.8s\n",
            "[CV] C=1.102112922044213, max_iter=1725.1867162187282 ................\n",
            "[CV] . C=1.102112922044213, max_iter=1725.1867162187282, total=  18.2s\n",
            "[CV] C=1.102112922044213, max_iter=1725.1867162187282 ................\n",
            "[CV] . C=1.102112922044213, max_iter=1725.1867162187282, total=  17.7s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  6.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "System took 21.607239246368408 seconds to model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.90      0.78     10659\n",
            "           1       0.84      0.48      0.61      7498\n",
            "           2       0.21      0.17      0.19      1445\n",
            "\n",
            "    accuracy                           0.69     19602\n",
            "   macro avg       0.58      0.52      0.52     19602\n",
            "weighted avg       0.71      0.69      0.67     19602\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxoZVtjKTo04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4cbc1179-1171-48ef-b13c-404dfc63e2b8"
      },
      "source": [
        "clf_svm"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRefLjtA6O_q",
        "colab_type": "text"
      },
      "source": [
        "Note that we used LinearSVC since Multiclass SVC using one-versus-one solver takes too long due to its O(n*2) complexity. Due to this, our accuracy for the third class will always be zero.\n",
        "\n",
        "**Artificial Neural Network Classifier / Multilayer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKpn0ffaCbxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98438969-ee4b-44c7-cd57-fd055ffc6e8c"
      },
      "source": [
        "import sklearn.neural_network\n",
        "clf_mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', \n",
        "                                                 alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, \n",
        "                                                 max_iter=1000, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
        "                                                 nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n",
        "                                                 n_iter_no_change=10)\n",
        "\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,),(150,),(200,),(50,), (200,100,50), (100,100,50), (100,100,100), (200,150,100), (200,200,200), (50,50,50,50)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
        "    'learning_rate_init' :[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
        "    'tol': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(clf_mlp, parameter_space, n_iter=10, verbose=2, cv=3)\n",
        "rnd_search_cv.fit(X_train, y_train)\n",
        "\n",
        "start_time = time.time()\n",
        "clf_mlp.fit(X_train,y_train)\n",
        "\n",
        "print(\"System took %s seconds to model\" % (time.time() - start_time))\n",
        "print(classification_report(y_test,clf_mlp.predict(X_test)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] tol=0.1, solver=sgd, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50, 50), alpha=0.001, activation=tanh \n",
            "[CV]  tol=0.1, solver=sgd, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50, 50), alpha=0.001, activation=tanh, total=   6.5s\n",
            "[CV] tol=0.1, solver=sgd, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50, 50), alpha=0.001, activation=tanh \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  tol=0.1, solver=sgd, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50, 50), alpha=0.001, activation=tanh, total=   6.4s\n",
            "[CV] tol=0.1, solver=sgd, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50, 50), alpha=0.001, activation=tanh \n",
            "[CV]  tol=0.1, solver=sgd, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50, 50), alpha=0.001, activation=tanh, total=   6.4s\n",
            "[CV] tol=0.0001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50), alpha=0.0001, activation=relu \n",
            "[CV]  tol=0.0001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50), alpha=0.0001, activation=relu, total=  22.4s\n",
            "[CV] tol=0.0001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50), alpha=0.0001, activation=relu \n",
            "[CV]  tol=0.0001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50), alpha=0.0001, activation=relu, total=  22.0s\n",
            "[CV] tol=0.0001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50), alpha=0.0001, activation=relu \n",
            "[CV]  tol=0.0001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(50, 50, 50), alpha=0.0001, activation=relu, total=  11.0s\n",
            "[CV] tol=0.0001, solver=sgd, learning_rate_init=0.05, learning_rate=adaptive, hidden_layer_sizes=(50,), alpha=0.0001, activation=tanh \n",
            "[CV]  tol=0.0001, solver=sgd, learning_rate_init=0.05, learning_rate=adaptive, hidden_layer_sizes=(50,), alpha=0.0001, activation=tanh, total=  58.8s\n",
            "[CV] tol=0.0001, solver=sgd, learning_rate_init=0.05, learning_rate=adaptive, hidden_layer_sizes=(50,), alpha=0.0001, activation=tanh \n",
            "[CV]  tol=0.0001, solver=sgd, learning_rate_init=0.05, learning_rate=adaptive, hidden_layer_sizes=(50,), alpha=0.0001, activation=tanh, total= 1.2min\n",
            "[CV] tol=0.0001, solver=sgd, learning_rate_init=0.05, learning_rate=adaptive, hidden_layer_sizes=(50,), alpha=0.0001, activation=tanh \n",
            "[CV]  tol=0.0001, solver=sgd, learning_rate_init=0.05, learning_rate=adaptive, hidden_layer_sizes=(50,), alpha=0.0001, activation=tanh, total= 1.7min\n",
            "[CV] tol=0.005, solver=sgd, learning_rate_init=0.05, learning_rate=constant, hidden_layer_sizes=(100,), alpha=0.05, activation=relu \n",
            "[CV]  tol=0.005, solver=sgd, learning_rate_init=0.05, learning_rate=constant, hidden_layer_sizes=(100,), alpha=0.05, activation=relu, total=  59.0s\n",
            "[CV] tol=0.005, solver=sgd, learning_rate_init=0.05, learning_rate=constant, hidden_layer_sizes=(100,), alpha=0.05, activation=relu \n",
            "[CV]  tol=0.005, solver=sgd, learning_rate_init=0.05, learning_rate=constant, hidden_layer_sizes=(100,), alpha=0.05, activation=relu, total= 1.0min\n",
            "[CV] tol=0.005, solver=sgd, learning_rate_init=0.05, learning_rate=constant, hidden_layer_sizes=(100,), alpha=0.05, activation=relu \n",
            "[CV]  tol=0.005, solver=sgd, learning_rate_init=0.05, learning_rate=constant, hidden_layer_sizes=(100,), alpha=0.05, activation=relu, total= 1.5min\n",
            "[CV] tol=0.001, solver=adam, learning_rate_init=0.0005, learning_rate=constant, hidden_layer_sizes=(200, 100, 50), alpha=0.001, activation=tanh \n",
            "[CV]  tol=0.001, solver=adam, learning_rate_init=0.0005, learning_rate=constant, hidden_layer_sizes=(200, 100, 50), alpha=0.001, activation=tanh, total= 3.0min\n",
            "[CV] tol=0.001, solver=adam, learning_rate_init=0.0005, learning_rate=constant, hidden_layer_sizes=(200, 100, 50), alpha=0.001, activation=tanh \n",
            "[CV]  tol=0.001, solver=adam, learning_rate_init=0.0005, learning_rate=constant, hidden_layer_sizes=(200, 100, 50), alpha=0.001, activation=tanh, total= 2.3min\n",
            "[CV] tol=0.001, solver=adam, learning_rate_init=0.0005, learning_rate=constant, hidden_layer_sizes=(200, 100, 50), alpha=0.001, activation=tanh \n",
            "[CV]  tol=0.001, solver=adam, learning_rate_init=0.0005, learning_rate=constant, hidden_layer_sizes=(200, 100, 50), alpha=0.001, activation=tanh, total= 1.6min\n",
            "[CV] tol=0.005, solver=adam, learning_rate_init=0.1, learning_rate=adaptive, hidden_layer_sizes=(50, 100, 50), alpha=0.0001, activation=relu \n",
            "[CV]  tol=0.005, solver=adam, learning_rate_init=0.1, learning_rate=adaptive, hidden_layer_sizes=(50, 100, 50), alpha=0.0001, activation=relu, total=   4.9s\n",
            "[CV] tol=0.005, solver=adam, learning_rate_init=0.1, learning_rate=adaptive, hidden_layer_sizes=(50, 100, 50), alpha=0.0001, activation=relu \n",
            "[CV]  tol=0.005, solver=adam, learning_rate_init=0.1, learning_rate=adaptive, hidden_layer_sizes=(50, 100, 50), alpha=0.0001, activation=relu, total=   4.9s\n",
            "[CV] tol=0.005, solver=adam, learning_rate_init=0.1, learning_rate=adaptive, hidden_layer_sizes=(50, 100, 50), alpha=0.0001, activation=relu \n",
            "[CV]  tol=0.005, solver=adam, learning_rate_init=0.1, learning_rate=adaptive, hidden_layer_sizes=(50, 100, 50), alpha=0.0001, activation=relu, total=   5.3s\n",
            "[CV] tol=0.1, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100,), alpha=0.0005, activation=tanh \n",
            "[CV]  tol=0.1, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100,), alpha=0.0005, activation=tanh, total=   4.8s\n",
            "[CV] tol=0.1, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100,), alpha=0.0005, activation=tanh \n",
            "[CV]  tol=0.1, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100,), alpha=0.0005, activation=tanh, total=   4.9s\n",
            "[CV] tol=0.1, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100,), alpha=0.0005, activation=tanh \n",
            "[CV]  tol=0.1, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100,), alpha=0.0005, activation=tanh, total=   4.5s\n",
            "[CV] tol=0.05, solver=sgd, learning_rate_init=0.0001, learning_rate=constant, hidden_layer_sizes=(100, 100, 50), alpha=0.005, activation=relu \n",
            "[CV]  tol=0.05, solver=sgd, learning_rate_init=0.0001, learning_rate=constant, hidden_layer_sizes=(100, 100, 50), alpha=0.005, activation=relu, total=   7.1s\n",
            "[CV] tol=0.05, solver=sgd, learning_rate_init=0.0001, learning_rate=constant, hidden_layer_sizes=(100, 100, 50), alpha=0.005, activation=relu \n",
            "[CV]  tol=0.05, solver=sgd, learning_rate_init=0.0001, learning_rate=constant, hidden_layer_sizes=(100, 100, 50), alpha=0.005, activation=relu, total=   7.5s\n",
            "[CV] tol=0.05, solver=sgd, learning_rate_init=0.0001, learning_rate=constant, hidden_layer_sizes=(100, 100, 50), alpha=0.005, activation=relu \n",
            "[CV]  tol=0.05, solver=sgd, learning_rate_init=0.0001, learning_rate=constant, hidden_layer_sizes=(100, 100, 50), alpha=0.005, activation=relu, total=   7.6s\n",
            "[CV] tol=0.001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(150,), alpha=0.05, activation=tanh \n",
            "[CV]  tol=0.001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(150,), alpha=0.05, activation=tanh, total=  15.7s\n",
            "[CV] tol=0.001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(150,), alpha=0.05, activation=tanh \n",
            "[CV]  tol=0.001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(150,), alpha=0.05, activation=tanh, total=  23.0s\n",
            "[CV] tol=0.001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(150,), alpha=0.05, activation=tanh \n",
            "[CV]  tol=0.001, solver=adam, learning_rate_init=0.01, learning_rate=constant, hidden_layer_sizes=(150,), alpha=0.05, activation=tanh, total=  17.8s\n",
            "[CV] tol=0.0001, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100, 100, 50), alpha=0.0001, activation=relu \n",
            "[CV]  tol=0.0001, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100, 100, 50), alpha=0.0001, activation=relu, total= 1.1min\n",
            "[CV] tol=0.0001, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100, 100, 50), alpha=0.0001, activation=relu \n",
            "[CV]  tol=0.0001, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100, 100, 50), alpha=0.0001, activation=relu, total=  37.9s\n",
            "[CV] tol=0.0001, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100, 100, 50), alpha=0.0001, activation=relu \n",
            "[CV]  tol=0.0001, solver=adam, learning_rate_init=0.0001, learning_rate=adaptive, hidden_layer_sizes=(100, 100, 50), alpha=0.0001, activation=relu, total=  28.7s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 19.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "System took 31.027844667434692 seconds to model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.89      0.81     10659\n",
            "           1       0.82      0.67      0.73      7498\n",
            "           2       0.51      0.26      0.34      1445\n",
            "\n",
            "    accuracy                           0.76     19602\n",
            "   macro avg       0.69      0.61      0.63     19602\n",
            "weighted avg       0.76      0.76      0.75     19602\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp0_kx99DfzM",
        "colab_type": "text"
      },
      "source": [
        "**Results - Unoptimized**\n",
        "\n",
        "We have XGBoost and MLP as our best performing models without any hyperparameter optimization. For references, we shall be running all of the models on the competition's test data and submit them to get a conclusive view of their effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty8OVawhUO2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f7dff757-57e3-4429-ff04-774e4f5553d7"
      },
      "source": [
        "clf_mlp"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI8hEcfH6Opm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "53297aae-cdaf-409e-99d9-83be1d3cc1df"
      },
      "source": [
        "'''\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "keras.layers.Dense(200, input_dim=287, activation='relu'),\n",
        "keras.layers.Dense(100, activation='relu'),\n",
        "keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\nmodel = keras.Sequential([\\nkeras.layers.Dense(200, input_dim=287, activation='relu'),\\nkeras.layers.Dense(100, activation='relu'),\\nkeras.layers.Dense(1, activation='sigmoid'),\\n])\\n\\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\\n\\nmodel.fit(X_train, y_train, epochs=10, batch_size=10)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5pFri3GEvTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finding the predictions from the models on the final test data\n",
        "xgb_pred = pd.DataFrame({'status_group' : clf_xgb.predict(test_data.drop(['id'],axis=1))})\n",
        "lr_pred = pd.DataFrame({'status_group' : clf_lr.predict(test_data.drop(['id'],axis=1))})\n",
        "svm_pred = pd.DataFrame({'status_group' : clf_svm.predict(test_data.drop(['id'],axis=1))})\n",
        "mlp_pred = pd.DataFrame({'status_group' : clf_mlp.predict(test_data.drop(['id'],axis=1))})\n",
        "\n",
        "# Adding the id column back to the predicitions\n",
        "id_df = pd.DataFrame(test_data['id'])\n",
        "xgb_pred = id_df.merge(xgb_pred,left_index=True,right_index=True)\n",
        "lr_pred = id_df.merge(lr_pred,left_index=True,right_index=True)\n",
        "svm_pred = id_df.merge(svm_pred,left_index=True,right_index=True)\n",
        "mlp_pred = id_df.merge(mlp_pred,left_index=True,right_index=True)\n",
        "\n",
        "\n",
        "# Creating Submission Files\n",
        "label_dict_status_group = {0:'functional', 1:'non functional', 2:'functional needs repair'}\n",
        "xgb_pred.status_group = xgb_pred.status_group.replace(label_dict_status_group)\n",
        "lr_pred.status_group = lr_pred.status_group.replace(label_dict_status_group)\n",
        "svm_pred.status_group = svm_pred.status_group.replace(label_dict_status_group)\n",
        "mlp_pred.status_group = mlp_pred.status_group.replace(label_dict_status_group)\n",
        "\n",
        "# Exporting the data to csv files for submission\n",
        "pd.DataFrame(xgb_pred).to_csv('xgb_pred.csv',index=False)\n",
        "pd.DataFrame(lr_pred).to_csv('lr_pred.csv',index=False)\n",
        "pd.DataFrame(svm_pred).to_csv('svm_pred.csv',index=False)\n",
        "pd.DataFrame(mlp_pred).to_csv('mlp_pred.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0b3LRhNVJGW",
        "colab_type": "text"
      },
      "source": [
        "We submitted the files to the portal and found that XGBoost performed the best followed by MLP and SVM. The scores are given below:\n",
        "\n",
        "> XGB received 0.7467\n",
        "\n",
        "> MLP Received 0.7459\n",
        "\n",
        "> SVM received 0.6927\n",
        "\n",
        "In the next notebook, we will optimize the parameters and discuss possible improvements to the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpSYBUV_WNGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}